{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, json, random, pickle\n",
    "random.seed(42)\n",
    "# ignore deprecation warnings in sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Specify data directory\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),'Data')\n",
    "\n",
    "# Set model directory\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'Model')\n",
    "\n",
    "# Set embedding directory\n",
    "\n",
    "embedding_dir = 'Z:\\Jupyter\\Embeddings'\n",
    "\n",
    "# Set data paths\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "\n",
    "train_processed_path = os.path.join(data_dir, 'interim', 'train_preprocessed.txt')\n",
    "\n",
    "meta_feat_path = os.path.join(data_dir, 'interim', 'meta_feat.txt')\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train_processed = pd.read_json(train_processed_path)\n",
    "meta_feat = pd.read_json(meta_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some support function to adjust class weights and \n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "class_weights = compute_sample_weight('balanced', \n",
    "                                      np.unique(train_processed.sentiment),\n",
    "                                      train_processed.sentiment)\n",
    "\n",
    "def get_label(row):\n",
    "    \"\"\"\n",
    "    Get regular label from one hot encoded labels\n",
    "    \"\"\"\n",
    "    for label in [0,1,2]:\n",
    "        if row[label] == 1:\n",
    "            return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Machine Learning with Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Blue'>Bi-Directional LSTM </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "\n",
    "vocab_size = 5000\n",
    "input_length = 120\n",
    "embed_dim = 100\n",
    "lstm_out = 100\n",
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3959, 120) (3959, 3)\n",
      "(1320, 120) (1320, 3)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and build model input\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, split=' ')\n",
    "tokenizer.fit_on_texts(train_processed['text'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_processed['text'].values)\n",
    "X = pad_sequences(X, maxlen = input_length)\n",
    "\n",
    "y = to_categorical(train_processed['sentiment'].values)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state = 42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Build Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embed_dim, \n",
    "                    input_length = X.shape[1], \n",
    "                    dropout=0.2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(lstm_out, \n",
    "                       dropout_U=0.2,\n",
    "                       dropout_W=0.2)))\n",
    "model.add(Dense(3,\n",
    "                activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(Xtrain, ytrain, \n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          class_weight = class_weights,\n",
    "          verbose = 0,\n",
    "          callbacks = [TQDMNotebookCallback()])\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir, \"LSTM_120inputlen_32bsize_5epoch.json\"), 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"LSTM_120inputlen_32bsize_5epoch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open(os.path.join(model_dir, \"LSTM_120inputlen_32bsize_5epoch.json\"), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"LSTM_120inputlen_32bsize_5epoch.h5\")\n",
    "print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 27  18  88]\n",
      " [ 22  50 154]\n",
      " [ 86  73 802]] \n",
      "\n",
      "\n",
      "Model achieve 0.425 F1 Macro Score\n"
     ]
    }
   ],
   "source": [
    "# Get model results\n",
    "ypred = model.predict(Xtest)\n",
    "\n",
    "# Turn probability into predictions\n",
    "ypred_df = pd.DataFrame(ypred)\n",
    "ypred_max = ypred_df.apply(max, axis = 1)\n",
    "for index, row in ypred_df.iterrows():\n",
    "    for label, item in row.items():\n",
    "        if item == ypred_max[index]:\n",
    "            row[label] = 1\n",
    "        else:\n",
    "            row[label] = 0\n",
    "\n",
    "# Get confusion matrix\n",
    "ypred_label = ypred_df.apply(get_label, axis = 1)\n",
    "ytest_label = pd.DataFrame(ytest).apply(get_label, axis = 1)\n",
    "\n",
    "cm = confusion_matrix(ytest_label, ypred_label)\n",
    "print(cm, \"\\n\\n\")\n",
    "\n",
    "f1 = f1_score(ytest_label, ypred_label, average = 'macro')\n",
    "\n",
    "print(\"Model achieve %.3f F1 Macro Score\" % f1)\n",
    "\n",
    "# Save score\n",
    "f1_lstm = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Blue'>LSTM with Pre-trained Embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "input_length = 120\n",
    "embed_dim = 100\n",
    "lstm_out = 100\n",
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37408 unique tokens.\n",
      "(3959, 120) (3959, 3)\n",
      "(1320, 120) (1320, 3)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words = vocab_size)\n",
    "tokenizer.fit_on_texts(train_processed['text'].values)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_processed['text'].values)\n",
    "X = pad_sequences(X, maxlen = input_length)\n",
    "\n",
    "y = to_categorical(train_processed['sentiment'].values)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state = 42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data into training and validation set\n",
    "indices = np.arange(Xtrain.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "Xtrain1 = Xtrain[indices]\n",
    "ytrain1 = ytrain[indices]\n",
    "nb_validation_samples = int(0.2 * Xtrain.shape[0])\n",
    "\n",
    "Xtrain1 = Xtrain1[:-nb_validation_samples]\n",
    "ytrain1 = ytrain1[:-nb_validation_samples]\n",
    "Xval = Xtrain1[-nb_validation_samples:]\n",
    "yval = ytrain1[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read GloVe embeddings\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(embedding_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM and run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, \n",
    "                    embed_dim, \n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length = X.shape[1], \n",
    "                    trainable = False))\n",
    "\n",
    "model.add(Bidirectional(LSTM(lstm_out, \n",
    "                       dropout_U=0.2,\n",
    "                       dropout_W=0.2)))\n",
    "model.add(Dense(3,\n",
    "                activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(Xtrain, ytrain,\n",
    "          epochs=num_epochs,\n",
    "          class_weight = class_weights,\n",
    "          verbose = 0,\n",
    "          callbacks = [TQDMNotebookCallback()])\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir, \"LSTM_pretrained_GloVe.json\"), 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"LSTM_pretrained_GloVe.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "json_file = open(os.path.join(model_dir, \"LSTM_pretrained_GloVe.json\"), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"LSTM_pretrained_GloVe.h5\")\n",
    "print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   1 131]\n",
      " [  0  11 215]\n",
      " [  2  12 947]] \n",
      "\n",
      "\n",
      "Model achieve 0.314 F1 Macro Score\n"
     ]
    }
   ],
   "source": [
    "# Get model results\n",
    "ypred = model.predict(Xtest)\n",
    "\n",
    "# Turn probability into predictions\n",
    "ypred_df = pd.DataFrame(ypred)\n",
    "ypred_max = ypred_df.apply(max, axis = 1)\n",
    "for index, row in ypred_df.iterrows():\n",
    "    for label, item in row.items():\n",
    "        if item == ypred_max[index]:\n",
    "            row[label] = 1\n",
    "        else:\n",
    "            row[label] = 0\n",
    "\n",
    "# Get confusion matrix\n",
    "ypred_label = ypred_df.apply(get_label, axis = 1)\n",
    "ytest_label = pd.DataFrame(ytest).apply(get_label, axis = 1)\n",
    "\n",
    "unique, counts = np.unique(ytest_label, return_counts = True)\n",
    "dict(zip(unique, counts))\n",
    "\n",
    "cm = confusion_matrix(ytest_label, ypred_label)\n",
    "print(cm, \"\\n\\n\")\n",
    "\n",
    "f1 = f1_score(ytest_label, ypred_label, average = 'macro')\n",
    "\n",
    "print(\"Model achieve %.3f F1 Macro Score\" % f1)\n",
    "\n",
    "# Save score\n",
    "f1_lstm_pretrained = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Blue'>LSTM with Word2Vec Embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "input_length = 120\n",
    "embed_dim = 200\n",
    "lstm_out = 100\n",
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37408 unique tokens.\n",
      "(3959, 120) (3959, 3)\n",
      "(1320, 120) (1320, 3)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words = vocab_size)\n",
    "tokenizer.fit_on_texts(train_processed['text'].values)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_processed['text'].values)\n",
    "X = pad_sequences(X, maxlen = input_length)\n",
    "\n",
    "y = to_categorical(train_processed['sentiment'].values)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state = 42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data into training and validation set\n",
    "indices = np.arange(Xtrain.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "Xtrain1 = Xtrain[indices]\n",
    "ytrain1 = ytrain[indices]\n",
    "nb_validation_samples = int(0.2 * Xtrain.shape[0])\n",
    "\n",
    "Xtrain1 = Xtrain1[:-nb_validation_samples]\n",
    "ytrain1 = ytrain1[:-nb_validation_samples]\n",
    "Xval = Xtrain1[-nb_validation_samples:]\n",
    "yval = ytrain1[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word2vec model\n",
    "w2v_model = Word2Vec.load(os.path.join(embedding_dir, 'w2v_best.bin'))\n",
    "\n",
    "print(w2v_model)\n",
    "\n",
    "word_vectors = w2v_model.wv\n",
    "print(\"Number of word vectors: %d\" %len(word_vectors.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word not in list(word_vectors.vocab.keys()):\n",
    "        embedding_matrix[i] = np.zeros(embed_dim)\n",
    "    else:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM and run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, \n",
    "                    embed_dim, \n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length = X.shape[1], \n",
    "                    trainable = False))\n",
    "\n",
    "model.add(Bidirectional(LSTM(lstm_out, \n",
    "                       dropout_U=0.2,\n",
    "                       dropout_W=0.2)))\n",
    "model.add(Dense(3,\n",
    "                activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(Xtrain, ytrain,\n",
    "          epochs=num_epochs,\n",
    "          class_weight = class_weights,\n",
    "          verbose = 0,\n",
    "          callbacks = [TQDMNotebookCallback()])\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir, \"LSTM_w2v.json\"), 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"LSTM_w2v.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "json_file = open(os.path.join(model_dir, \"LSTM_w2v.json\"), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"LSTM_w2v.h5\")\n",
    "print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11   7 115]\n",
      " [  3  39 184]\n",
      " [ 29  42 890]] \n",
      "\n",
      "\n",
      "Model achieve 0.400 F1 Macro Score\n"
     ]
    }
   ],
   "source": [
    "# Get model results\n",
    "ypred = model.predict(Xtest)\n",
    "\n",
    "# Turn probability into predictions\n",
    "ypred_df = pd.DataFrame(ypred)\n",
    "ypred_max = ypred_df.apply(max, axis = 1)\n",
    "for index, row in ypred_df.iterrows():\n",
    "    for label, item in row.items():\n",
    "        if item == ypred_max[index]:\n",
    "            row[label] = 1\n",
    "        else:\n",
    "            row[label] = 0\n",
    "\n",
    "# Get confusion matrix\n",
    "ypred_label = ypred_df.apply(get_label, axis = 1)\n",
    "ytest_label = pd.DataFrame(ytest).apply(get_label, axis = 1)\n",
    "\n",
    "unique, counts = np.unique(ytest_label, return_counts = True)\n",
    "dict(zip(unique, counts))\n",
    "\n",
    "cm = confusion_matrix(ytest_label, ypred_label)\n",
    "print(cm, \"\\n\\n\")\n",
    "\n",
    "f1 = f1_score(ytest_label, ypred_label, average = 'macro')\n",
    "\n",
    "print(\"Model achieve %.3f F1 Macro Score\" % f1)\n",
    "\n",
    "# Save score\n",
    "f1_lstm_w2v = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Blue'>LSTM results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>F1-macro-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vanilla LSTM</td>\n",
       "      <td>0.424657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM with Custom-trained Word2Vec Embeddings</td>\n",
       "      <td>0.400438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM With Pre-trained Glove Embeddings</td>\n",
       "      <td>0.314330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  F1-macro-score\n",
       "0                                  Vanilla LSTM        0.424657\n",
       "2  LSTM with Custom-trained Word2Vec Embeddings        0.400438\n",
       "1        LSTM With Pre-trained Glove Embeddings        0.314330"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\"model\":[\"Vanilla LSTM\", \"LSTM With Pre-trained Glove Embeddings\", \"LSTM with Custom-trained Word2Vec Embeddings\"],\n",
    "           \"F1-macro-score\":[f1_lstm, f1_lstm_pretrained, f1_lstm_w2v]}\n",
    "pd.DataFrame(results).sort_values(\"F1-macro-score\", ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
