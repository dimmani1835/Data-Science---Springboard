{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules necessary for the spam filter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data directory and path path\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'Data')\n",
    "data_path = os.path.join(data_dir,'SMSSpamCollection.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(data_path, delimiter = '\\t', header = None)\n",
    "df_raw.columns = ['label', 'text']\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Zach\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zach\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Zach Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Zach Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Define cleaning modules and cleaning functions\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "# Import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "# Create stopwords list        \n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Define lemmatizing functions\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" \n",
    "    Conduct pre-processing, tag words then returns sentence with lemmatized words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty list of lemmatized tokens\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    \n",
    "    # Iterate over sentences to conduct lemmatization\n",
    "    for sentence in tokenized_sent:\n",
    "        \n",
    "        # Tokenize the words in the sentence\n",
    "        tokenized_word = word_tokenize(sentence)\n",
    "        \n",
    "        # Tag the pos of the tokens\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        \n",
    "        # Initialize a empty list of lemmatized words\n",
    "        root = []\n",
    "\n",
    "        # Create Lemmatizer object\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        # iterate over the tagged sentences to \n",
    "        for token in tagged_token:\n",
    "\n",
    "            # assign tag and actual word of the token\n",
    "            tag = token[1][0]\n",
    "            word = token[0]\n",
    "\n",
    "            # Lemmatize the token based on tags\n",
    "            if tag.startswith('J'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "            elif tag.startswith('V'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "            elif tag.startswith('N'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "            elif tag.startswith('R'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "            else:          \n",
    "                root.append(word)\n",
    "\n",
    "        # Add the lemmatized word into our list\n",
    "        lemmatized_list.extend(root)\n",
    "        \n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \"\"\"\n",
    "    Expand the contractions form to create cohenrent extractions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Substitute quotation marks with apostrophes\n",
    "    text = re.sub(\"â€™\", \"'\", text)\n",
    "    \n",
    "    # define the contraction pattern with custom contraction mappings\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    # Define function to expand contraction matches\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# Define main text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Return a processed version of the text given\n",
    "    \"\"\"\n",
    "    # Turn all text into lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand all contractions\n",
    "    text = expand_contractions(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    \n",
    "    # Remove all punctuations\n",
    "    #punctuations = '''!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'''\n",
    "    text = ' '.join(word.strip(string.punctuation) for word in text.split())\n",
    "    \n",
    "    # Remove numerics\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Lemmatize text\n",
    "    text = lemmatize_doc(text)\n",
    "    \n",
    "    # Removing Extra spaces if any\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    \n",
    "    # Convert \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5572/5572 [00:10<00:00, 517.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df_processed = df_raw.text.progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, fbeta_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_figures(clf, Xtrain, Xtest, ytrain, ytest, best_beta):\n",
    "    # Derive scores\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    f1 = f1_score(ytest, clf.predict(Xtest), average = 'macro')\n",
    "    fbeta = fbeta_score(ytest, clf.predict(Xtest), beta = best_beta)\n",
    "    probs = clf.predict_proba(Xtest)[:, 1]\n",
    "    auc = roc_auc_score(ytest, probs)\n",
    "    print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "    print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n",
    "    print(\"AUC-ROC score     {:2f}\".format(auc))\n",
    "    print(\"F1 score     {:2f}\".format(f1))\n",
    "    print(\"Fbeta score for beta = {:2f} is {:2f}\".format(round(best_beta,3), fbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MetaDataExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in text series, outputs meta-features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_meta_data(self, message):\n",
    "        \"\"\"\n",
    "        This function preprocess one text message, creating a list of metadata.\n",
    "        The message argument is a message.\n",
    "        \"\"\"\n",
    "\n",
    "        # Replace email addresses with 'EmAd'\n",
    "        message = re.sub(r'[^\\s]+@.[^\\s]+', '{EmAd}', message)\n",
    "\n",
    "        # Replace URLs with 'Url'\n",
    "        message = re.sub(r'http[^\\s]+', '{Url}', message)\n",
    "\n",
    "        # Replace money symbols with 'MoSy'\n",
    "        message = re.sub(r'Â£|\\$', '{MoSy}', message)\n",
    "\n",
    "        # Replace 10 or 11 digit phone numbers\n",
    "        message = re.sub(r'0?(\\d{10,}?)','{PhNu}', message)\n",
    "\n",
    "        # Derive tokens\n",
    "        token = nltk.word_tokenize(message)\n",
    "\n",
    "        # Derive number of tokens\n",
    "        n_token = len(token)\n",
    "\n",
    "        # Derive the average length of a token\n",
    "        avg_len = np.mean([len(word) for word in message.split()])\n",
    "\n",
    "        # Derive the number of numerics\n",
    "        n_num = len([tok for tok in message if tok.isdigit() or tok == '{PhNu}'])\n",
    "\n",
    "        # Derive if the message has numerics\n",
    "        has_num = np.where(n_num > 0,1,0)\n",
    "\n",
    "        # Derive the number of uppercased words\n",
    "        n_uppers = len([word for word in message if word.isupper()])\n",
    "\n",
    "        # Derive the number of English stop words\n",
    "        n_stops = len([word for word in message if word in stopwords.words('english')])\n",
    "\n",
    "        # Derive the symbol columns\n",
    "        has_email = np.where('{EmAd}' in message,1, 0)\n",
    "        has_money = np.where('{MoSy}' in message,1, 0)\n",
    "        has_phone = np.where('{PhNu}' in message,1,0)\n",
    "        has_url = np.where('{Url}' in message,1,0)\n",
    "\n",
    "        return n_token, avg_len, n_num, has_num, n_uppers, n_stops, has_email, has_money, has_phone, has_url\n",
    "    \n",
    "    def transform(self, messages, y=None):\n",
    "        \"\"\"\n",
    "        Tranform the meta-data features extracted and convert into dataframe format\n",
    "        \"\"\"\n",
    "        return messages.apply(self.extract_meta_data).progress_apply(pd.Series)\n",
    "\n",
    "    def fit(self, messages, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words = 'english', \n",
    "                        ngram_range = (1, 2), \n",
    "                        token_pattern=r'\\b\\w+\\b',\n",
    "                        min_df = 1)\n",
    "vec = vec.fit(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'best_vec.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5572/5572 [00:01<00:00, 4943.91it/s]\n"
     ]
    }
   ],
   "source": [
    "sparse_feat = vec.transform(df_processed)\n",
    "\n",
    "dense_feat = MetaDataExtractor().fit_transform(df_raw.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feat = coo_matrix(MinMaxScaler().fit_transform(dense_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([sparse_feat, dense_feat.astype(float)])\n",
    "y = (df_raw.label == 'spam').values.astype(np.int)\n",
    "indices = df_raw.index\n",
    "\n",
    "# Split train and test set\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha = 5)\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.991474\n",
      "Accuracy on test data:     0.990135\n",
      "AUC-ROC score     0.978032\n",
      "F1 score     0.978263\n",
      "Fbeta score for beta = 0.100000 is 0.985435\n"
     ]
    }
   ],
   "source": [
    "get_score_figures(clf, Xtrain, Xtest, ytrain, ytest, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[964,   2],\n",
       "       [  9, 140]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ytest, clf.predict(Xtest), labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'best_model.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'rb') as f:\n",
    "    clf_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_loaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
